name: Add paper (Issue âžœ data/papers.json)

on:
  issues:
    types: [opened, edited, reopened, labeled]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  add-paper:
    # Only run when the issue has the "add-paper" label (your form adds it)
    #if: contains(github.event.issue.labels.*.name, 'add-paper')
    if: contains(github.event.issue.labels.*.name, 'add-paper') || contains(github.event.issue.body, 'Paper URL')
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      # Parse the Issue Form body (URL + optional fields)
      - id: parse
        uses: actions/github-script@v7
        with:
          script: |
            const body = (context.payload.issue.body || "");
            // 1) first URL found
            const url = (body.match(/https?:\/\/\S+/) || [])[0] || "";
            if (!url) core.setFailed("No URL found in issue body");

            // 2) "Type" section (dropdown) -> first word on following line
            function sectionValue(title){
              const re = new RegExp(`###\\s*${title}\\s*\\n+([^\\n]+)`, "i");
              const m = body.match(re); 
              return (m ? m[1].trim() : "");
            }
            const type = (sectionValue("Type") || "").toLowerCase();

            // 3) Domains are checkboxes like "- [x] genomics"
            const domains = [...body.matchAll(/- \[x\]\s+([A-Za-z0-9_-]+)/g)]
              .map(m => m[1].toLowerCase());

            // 4) Free text comma lists
            function csv(title){
              const v = sectionValue(title).toLowerCase();
              return v ? v.split(/[;,]/).map(s => s.trim()).filter(Boolean) : [];
            }
            const modality = csv("Modality tags");
            const task     = csv("Task tags");

            // 5) Optional year
            const year = (sectionValue("Year").match(/\d{4}/) || [])[0] || "";

            core.setOutput("url", url);
            core.setOutput("type", type);
            core.setOutput("domains", JSON.stringify(domains));
            core.setOutput("modality", JSON.stringify(modality));
            core.setOutput("task", JSON.stringify(task));
            core.setOutput("year", year);

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python deps
        run: pip install requests feedparser

      - name: Append/update data/papers.json
        env:
          URL:       ${{ steps.parse.outputs.url }}
          TYPE:      ${{ steps.parse.outputs.type }}
          DOMAINS:   ${{ steps.parse.outputs.domains }}
          MODALITY:  ${{ steps.parse.outputs.modality }}
          TASK:      ${{ steps.parse.outputs.task }}
          FIXED_YEAR:${{ steps.parse.outputs.year }}
        run: |
          python - <<'PY'
          import os, re, json, pathlib, requests, feedparser

          url       = os.environ["URL"]
          type_val  = os.environ.get("TYPE","").strip()
          domains   = json.loads(os.environ.get("DOMAINS","[]"))
          modality  = json.loads(os.environ.get("MODALITY","[]"))
          task      = json.loads(os.environ.get("TASK","[]"))
          fixed_year= os.environ.get("FIXED_YEAR","").strip()

          data_path = pathlib.Path("data/papers.json")
          data_path.parent.mkdir(parents=True, exist_ok=True)
          if data_path.exists():
              papers = json.loads(data_path.read_text(encoding="utf-8"))
          else:
              papers = []

          # -------- helpers --------
          def doi_from_nature(u):
              m=re.search(r"/articles/([^/?#]+)", u)
              return f"10.1038/{m.group(1)}" if m else None

          def doi_from_host(u, host):
              if "ai.nejm.org" in host or "nejm.org" in host or "acpjournals.org" in host \
                 or "pnas.org" in host or "science.org" in host or "doi.org" in host:
                  m=re.search(r"(10\.\d{4,9}/[^\s#]+)", u)
                  return m.group(1) if m else None
              if "nature.com" in host:
                  return doi_from_nature(u)
              return None

          def crossref(doi):
              r=requests.get(f"https://api.crossref.org/works/{doi}", timeout=25)
              r.raise_for_status()
              msg=r.json()["message"]
              title=(msg.get("title") or [""])[0]
              authors=[]
              for a in msg.get("author",[]):
                  nm=" ".join(x for x in [a.get("given"), a.get("family")] if x)
                  if nm: authors.append(nm)
              container = (msg.get("container-title") or [None])[0] or msg.get("publisher","")
              dp = (msg.get("issued",{}).get("date-parts") or [[None]])[0]
              year=dp[0]
              month=dp[1] if len(dp)>1 else 1
              day=dp[2] if len(dp)>2 else 1
              date=f"{year:04d}-{(month or 1):02d}-{(day or 1):02d}" if year else None
              return {"title":title, "authors": authors[:10] + (["et al."] if len(authors)>10 else []),
                      "venue":container, "date":date, "year":year}

          def arxiv_meta(arx_id):
              feed=feedparser.parse(f"http://export.arxiv.org/api/query?id_list={arx_id}")
              if not feed.entries: return None
              e=feed.entries[0]
              dt=e.published.split("T")[0]
              return {"title":e.title, "authors":[a.name for a in e.authors][:10] + (["et al."] if len(e.authors)>10 else []),
                      "venue":"arXiv", "date":dt, "year":int(dt[:4])}

          # -------- fetch metadata --------
          meta={"title":url, "authors":[], "venue":"", "date":None, "year": int(fixed_year) if fixed_year else None}
          host=re.sub(r"^https?://","",url).split("/")[0].lower()

          if "arxiv.org/abs/" in url:
              arx_id=url.rsplit("/",1)[-1]
              info=arxiv_meta(arx_id)
              if info: meta.update(info)
          else:
              doi=doi_from_host(url, host)
              if doi:
                  try:
                      info=crossref(doi)
                      meta.update(info)
                  except Exception:
                      pass  # keep minimal meta if Crossref fails

          # -------- assemble entry --------
          entry={
              "title": meta["title"],
              "authors": meta["authors"],
              "venue": meta["venue"],
              "date": meta["date"],
              "year": meta["year"],
              "url": url,
              "code": "",
              "abstract": "",
              "type": [type_val] if type_val else ["original"],
              "domain": domains,
              "modality": modality,
              "system": [],
              "task": task
          }

          # Upsert by URL
          found=False
          for i,p in enumerate(papers):
              if p.get("url")==url:
                  papers[i]=entry
                  found=True
                  break
          if not found:
              papers.append(entry)

          data_path.write_text(json.dumps(papers, indent=2, ensure_ascii=False), encoding="utf-8")
          print(f"Wrote {len(papers)} entries to {data_path}")
          PY

      # Create a PR (safer). If you want direct commit to main, see comment below.
      - name: Create pull request
        uses: peter-evans/create-pull-request@v6
        with:
          commit-message: "chore: add paper from issue #${{ github.event.issue.number }}"
          title: "Add paper from issue #${{ github.event.issue.number }}"
          body: "Auto-generated from issue #${{ github.event.issue.number }}."
          branch: "add-paper/${{ github.event.issue.number }}"
          labels: add-paper
